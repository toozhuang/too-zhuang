---
title: http的队头阻塞和多路复用
date: "2019-10-09 22:14:28"
tags: ["web"]
categories: readings
---





![](https://raw.githubusercontent.com/wangtoday/Picturebed/master/O6p11y.jpg)





## http中队头阻塞和多路复用

![](https://raw.githubusercontent.com/wangtoday/Picturebed/master/iW3j2p.png)

看到网上有人面试过这个问题， **恰好我也不知道**

上面那张图， 不是很清晰。

图1第一种请求方式就是单次发送request请求，收到response后再进行下一次请求。 **低效**

于是 http1.1 提出了 管线化技术（pipelining）技术，就是第二个方式，一次性发送多个request的请求。而且这个pipelining再接受response返回的时候，也必须依顺序接受，如果前一个请求遇到阻塞，后面的请求即使已经处理完毕，仍然需要等待阻塞的请求处理完毕。 

也就是第二个和第三个图的情形， 显示的就是队的头部阻塞。

为了解决上述阻塞问题， http2中提出了多路复用的技术（multiplexing）。 http2中将多个请求复用同一个tcp连接中， 将一个tcp连接分成若干个流，每个流可以传输若干个消息。

------

上面的讲法还是有一点模式， 我们转战第二个博文看看吧， 

http在响应上一个请求之前往往无法接受新的请求。因此网络延迟和请求的复杂性决定了其空闲程度。值得注意的是，在http/1.1支持流水线操作，但是仍然没有解决这种阻塞的问题。（如上图1）



**还是不太懂， 又找了一个博文-  - #**



首先， 关于http队头阻塞的问题。 

Http1.1通过pipelining管道技术实现了一次性发送多个请求，以提高吞吐量和性能，但是上图2显示有一个阻塞等待的问题。

http2的方式是数据分帧， 多个请求复用同一个tcp连接，然后每个request-response都被拆分成若干个frame发送，这样即使一个请求被阻塞了，也不会影响其他的请求，如上面的最后一个图。 

但是如果队头阻塞的界别是tcp层面的阻塞， 那么就没办法了。 目前的这个解决方法是http request级别的解决方法。

## 深层次的解释队头阻塞

#### 先说一下tcp层面的队头阻塞

队头阻塞（head-of-line blocking）发生在一个TCP分节丢失，导致其后续分节不按序到达接收端的时候。该后续分节将被接收端一直保持直到丢失的第一个分节被发送端重传并到达接收端为止「这个地方的意思是，如果有a,b,c三个传输，a出了问题，虽然b和c都能接着发送到接受端，但是接受端会先保持这个b和c的发送信息，一直到a完美的重新发送过去才会进行b和c的处理操作」。该后续分节的延迟递送确保接收应用进程能够**按照发送端的发送顺序接收数据**。这种为了达到完全有序而引入的延迟机制非常有用，但也有不利之处。

假设在单个TCP连接上发送语义**独立**的消息，比如说服务器可能发送3幅不同的图像供Web浏览器显示。为了营造这几幅图像在用户屏幕上并行显示的效果，服务器先发送第一幅图像的一个断片，再发送第二幅图像的一个断片，然后再发送第三幅图像的一个断片；服务器重复这个过程，直到这3幅图像全部成功地发送到浏览器为止。

要是第一幅图像的某个断片内容的TCP分节丢失了，**客户端将保持已到达的不按序的所有数据**，直到丢失的分节重传成功。这样不仅延缓了第一幅图像数据的递送，也延缓了第二幅和第三幅图像数据的递送。

#### http队头阻塞

要介绍HTTP队头阻塞，就需要先讲讲HTTP的管道化（pipelining）。

##### http管道化是什么

管道化和非管道化的图示：

![](https://raw.githubusercontent.com/wangtoday/Picturebed/master/TRpyaM.jpg)

http1.1中， 在响应到达之前，可以将多个请求放到队列（如右图），当第一条请求发往服务器的时候，二三条也可以发送。

##### http管道化产生的背景

在一般情况下，HTTP遵守“请求-响应”的模式，也就是客户端每次发送一个请求到服务端，服务端返回响应。这种模式非常容易理解，但是效率并不是那么高，为了提高速度和效率，人们做了很多尝试：

- 最简单的情况下，服务端一旦返回响应后就会把对应的连接关闭，客户端的多个请求实际上是串行发送的。
- 除此之外，客户端可以选择同时创建多个连接，在多个连接上并行的发送不同请求。但是创建更多连接也带来了更多的消耗，当前大部分浏览器都会限制对同一个域名的连接数。
- 从HTTP1.0开始增加了持久连接的概念（HTTP1.0的Keep-Alive和HTTP1.1的persistent），可以使HTTP能够复用已经创建好的连接。客户端在收到服务端响应后，可以复用上次的连接发送下一个请求，而不用重新建立连接。
- 现代浏览器大多采用并行连接与持久连接共用的方式提高访问速度，对每个域名建立并行地少量持久连接。
- 而在持久连接的基础上，HTTP1.1进一步地支持在持久连接上使用管道化（pipelining）特性。管道化允许客户端在已发送的请求收到服务端的响应之前发送下一个请求，借此来减少等待时间提高吞吐；如果多个请求能在同一个TCP分节发送的话，还能提高网络利用率。但是因为HTTP管道化本身可能会导致队头阻塞的问题，以及一些其他的原因，现代浏览器默认都关闭了管道化。

##### 管道化的限制

1. 管道化要求服务端按照**请求发送的顺序返回响应（FIFO）**，原因很简单，HTTP请求和响应并没有序号标识，无法将乱序的响应与请求关联起来。
2. 客户端需要保持未收到响应的请求，当连接意外中断时，需要重新发送这部分请求。
3. 只有幂等的请求才能进行管道化，也就是只有GET和HEAD请求才能管道化，否则可能会出现意料之外的结果。 这个地方是和2相辅相成的。 因为幂等http方法代表的是同一个请求被执行一次或者执行多次的效果是一样的，服务器的状态也是一样的换句话说，幂等不应该具有副作用。而2里面表示的就是如果管道中的某一个请求没有收到回应，那么还要重新发送这部分的请求。

> ##### 根据上面管道化的限制，就能得到管道化会有一个队头阻塞的副作用。

### 如何解决队头阻塞？

对于HTTP1.1中管道化导致的**请求/响应级别的队头阻塞**，可以使用HTTP2解决。**HTTP2不使用管道化的方式，而是引入了帧、消息和数据流等概念**，*每个请求/响应被称为消息，每个消息都被拆分成若干个帧进行传输*，**每个帧都分配一个序号**。每个帧在传输是属于一个数据流，而一个连接上可以存在多个流，各个帧在流和连接上独立传输，**到达之后在组装成消息**「这个组装，就是根据每个帧的序号来组装的」，这样就避免了请求/响应阻塞。

当然，即使使用HTTP2，如果HTTP2底层使用的是TCP协议，仍可能出现TCP队头阻塞。

### 如何解决tcp队头阻塞？

TCP中的队头阻塞的产生是由TCP自身的实现机制决定的，**无法避免**。想要在应用程序当中避免TCP队头阻塞带来的影响，只有舍弃TCP协议。

比如google推出的[quic](https://link.juejin.im/?target=https%3A%2F%2Fkm.sankuai.com%2Fpage%2F108687233)协议，在某种程度上可以说避免了TCP中的队头阻塞，因为它根本不使用TCP协议，而是在UDP协议的基础上实现了可靠传输。而UDP是面向数据报的协议，数据报之间不会有阻塞约束。

此外还有一个SCTP（流控制传输协议），它是和TCP、UDP在同一层次的传输协议。SCTP的多流特性也可以尽可能的避免队头阻塞的情况。



### 总结：

从TCP队头阻塞和HTTP队头阻塞的原因我们可以看到，出现队头阻塞的原因有两个：

1. 独立的消息数据都在一个链路上传输，也就是有一个“队列”。比如TCP只有一个流，多个HTTP请求共用一个TCP连接
2. 队列上传输的数据有严格的顺序约束。比如**TCP要求数据严格按照序号顺序**，**HTTP管道化要求响应严格按照请求顺序返回**

所以要避免队头阻塞，就需要从以上两个方面出发，比如quic协议不使用TCP协议而是使用UDP协议，SCTP协议支持一个连接上存在多个数据流等等。











